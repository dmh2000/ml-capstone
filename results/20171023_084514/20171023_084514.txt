timestamp : 20171023_084514
config    : config/model3/level1-N39W120.json
datafile  : data/level1/N39W120.hgt
divisor   : 16
augments  : 15
epochs    : 300
model     : model3
input shape      : (1200, 1200)
subdivided shape : (256, 75, 75)
normalized shape : (256, 75, 75)
X shape          : (4096, 75, 75, 1)
y shape          : (4096,)
model init
train data      X: (3072, 75, 75, 1) y: (3072, 256)
validation data X: (256, 75, 75, 1) y: (256, 256)
test data       X: (256, 75, 75, 1) y: (256, 256)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 75, 75, 32)        832       
_________________________________________________________________
dropout_1 (Dropout)          (None, 75, 75, 32)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 37, 37, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 37, 37, 32)        25632     
_________________________________________________________________
dropout_2 (Dropout)          (None, 37, 37, 32)        0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 18, 18, 32)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 18, 18, 32)        25632     
_________________________________________________________________
dropout_3 (Dropout)          (None, 18, 18, 32)        0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 9, 9, 32)          0         
_________________________________________________________________
global_average_pooling2d_1 ( (None, 32)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 256)               8448      
=================================================================
Total params: 60,544
Trainable params: 60,544
Non-trainable params: 0
_________________________________________________________________
<<START>>
..................................................
..................................................
..................................................
..................................................
..................................................
..................................................

<<DONE>>

loss : 0.08486463688313961
acc  : 0.97265625
 32/256 [==>...........................] - ETA: 0sfinal accuracy : 0.9928
best  accuracy : 0.9967
final loss     : 0.0398
best loss      : 0.0144
final val acc  : 0.9531
best  val acc  : 0.9922
final val loss : 0.2138
best  val loss : 0.0390
Avg Accuracy   : 97.2656%
epochs         : 300
training time  : 14:21
[0 0][1 1][2 2][3 3][4 4][5 5][6 6][7 7][8 8][9 9][10 10][11 11][12 12][13 13][14 14][15 15]
[16 16]
[17 17]
[18 18]
[19 19]
[20 20]
[21 21]
[22 22]
[23 23]
[24 24]
[ 25 148]
[26 26]
[27 27]
[28 28]
[29 29]
[30 30]
[31 31]
[32 32]
[33 33]
[34 34]
[35 35]
[36 36]
[37 37]
[38 38]
[39 39]
[40 40]
[41 84]
[42 42]
[43 43]
[44 44]
[45 45]
[46 46]
[47 47]
[48 48]
[49 49]
[ 50 239]
[51 51]
[52 52]
[53 53]
[54 54]
[55 55]
[56 56]
[57 57]
[58 58]
[59 59]
[60 60]
[61 61]
[62 62]
[63 63]
[64 64]
[65 65]
[66 66]
[67 67]
[68 68]
[69 69]
[70 70]
[71 71]
[72 72]
[73 73]
[74 74]
[75 75]
[76 76]
[77 77]
[78 78]
[79 79]
[80 80]
[81 81]
[82 82]
[83 83]
[84 84]
[85 85]
[86 86]
[87 87]
[88 88]
[89 89]
[90 90]
[91 91]
[92 92]
[93 93]
[94 94]
[95 95]
[96 96]
[97 97]
[98 98]
[99 99]
[100 100]
[101 101]
[102 102]
[103 103]
[104 104]
[105 105]
[106 106]
[107 107]
[108 108]
[109 109]
[110 110]
[111 111]
[112 112]
[113 113]
[114 114]
[115 115]
[116 116]
[117 117]
[118 118]
[119 119]
[120 120]
[121 121]
[122 122]
[123 123]
[124 124]
[125 125]
[126 126]
[127 127]
[128 128]
[129 129]
[130 130]
[131 131]
[132 132]
[133 133]
[134 134]
[135 135]
[136 136]
[137 137]
[138 138]
[139 139]
[140 140]
[141 141]
[142 142]
[143 143]
[144 144]
[145 145]
[146 146]
[147 147]
[148 148]
[149 149]
[150 150]
[151 151]
[152 152]
[153 153]
[154 154]
[155 155]
[156 156]
[157 157]
[158 158]
[159 159]
[160 160]
[161 161]
[162 162]
[163 163]
[164 164]
[165 165]
[166 166]
[167 167]
[168 168]
[169 169]
[170 170]
[171 171]
[172 172]
[173 173]
[174 174]
[175 175]
[176 176]
[177 177]
[178 178]
[179 179]
[180 164]
[181 181]
[182 182]
[183 183]
[184 184]
[185 185]
[186 186]
[187 187]
[188 188]
[189  94]
[190 190]
[191 191]
[192 192]
[193 193]
[194 194]
[195 195]
[196 196]
[197 197]
[198 198]
[199 199]
[200 200]
[201 201]
[202 202]
[203 203]
[204 204]
[205 205]
[206 206]
[207 204]
[208 208]
[209 209]
[210 210]
[211 211]
[212 212]
[213 213]
[214 214]
[215 215]
[216 216]
[217 217]
[218 218]
[219 219]
[220 220]
[221 221]
[222 222]
[223 223]
[224 224]
[225 225]
[226 226]
[227 227]
[228 228]
[229 229]
[230  96]
[231 231]
[232 232]
[233 233]
[234 234]
[235 235]
[236 236]
[237 237]
[238 238]
[239 239]
[240 240]
[241 241]
[242 242]
[243 243]
[244 244]
[245 245]
[246 246]
[247 247]
[248 248]
[249 249]
[250 250]
[251 251]
[252 252]
[253 253]
[254 254]
[255 255]

done
