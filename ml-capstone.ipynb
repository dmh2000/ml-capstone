{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "\n",
    "## Capstone Project\n",
    "\n",
    "David Howard\n",
    "\n",
    "10/15/2017\n",
    "\n",
    "## I. Definition\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "SLAM (simultaneous location and mapping) is a well-known technique for using sensor data to create a \n",
    "map of a geographic region and also keep track of the sensors location within the constructed map. \n",
    "Typically this is done with either a particle filter or a Kalman filter. One application of SLAM \n",
    "operates on an unknown region and builds up knowledge about the region via sensor measurements.\n",
    " Another application uses existing knowledge, such as maps, along with sensors, and is aimed at \n",
    " determining the sensor&#39;s location within the known region. [[1]](https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping)\n",
    "\n",
    "In reading about SLAM, I found it interesting that I had previously been trained to be a human SLAM \n",
    "algorithm when I was a helicopter pilot in the U.S. Army. We trained to navigate high speed low level \n",
    "flight by using only a topographic map and our eyes observing the terrain elevation features (no GPS allowed).\n",
    "There is a name for this technique, Nap-Of-the-Earth flight.\n",
    " [[2]](https://en.wikipedia.org/wiki/Nap-of-the-earth#Helicopter_NOE_flying)\n",
    "\n",
    "One source of potential data for use in a SLAM system is a digital elevation model, \n",
    "generically referred to as DEM. A great source of such data was created by the NASA \n",
    "Shuttle Radar Topography Mission (SRTM). In 2000, the Space Shuttle was used to create a \n",
    "digital elevation map of the entire world (minus some polar regions). The NASA SRTM data is \n",
    "publicly available for download, in various height resolutions. [[3]](https://www2.jpl.nasa.gov/srtm/mission.htm))\n",
    "\n",
    "**Problem Statement**\n",
    "\n",
    "This project will attempt to create a simple SLAM type implementation using a convolutional neural \n",
    "network to determine position within a known region. The train/test data used for learning will be \n",
    "SRTM elevation data treated as an image. Input for evaluation will be images composed of pieces of \n",
    "the same data with possible modifications such as distortion, alignment and reduced resolution. \n",
    "Output will be a predicted position of the input images. The metric will be accuracy of \n",
    "the predictions over a test data set.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "The outputs of the model will be predictions of the probabilities of a test image \n",
    "location matching one of the actual locations. The outputs will be a set of probabilities, \n",
    "like the dog project. The &#39;absolute&#39; accuracy will be the average of the highest predicted \n",
    "probability being correct over the set of test data. The evaluation will also attempt to evaluate \n",
    "if the actual location matches any one of the higher of the list of predicted probabilities, it if isn't the highest.\n",
    "\n",
    "## II. Analysis\n",
    "\n",
    "_(approx. 2-4 pages)_\n",
    "\n",
    "### Data Exploration\n",
    "\n",
    "SRTM data is available online at [[4]](https://dds.cr.usgs.gov/srtm).\n",
    "\n",
    "SRTM data is available in multiple resolutions. The data is segmented into 1 latitude/longitude degree squares. \n",
    "The highest publicly available resolution is 1 arc-second per elevation posting (Level 2, ~30 meters at the equator), \n",
    "which results in a 3600x3600 matrix of elevations. The data files are actually 3601x3601 in order to fill overlaps if \n",
    "multiple cells are composed together. Other resolutions are 3 arc seconds (Level 1, ~90 meters) and (Level 0, 30 arc seconds). \n",
    "A detailed description of the data is available at [[5]](https://dds.cr.usgs.gov/srtm/version2\\_1/Documentation/SRTM\\_Topo.pdf)\n",
    "\n",
    "Organization of data\n",
    "\n",
    "- Level 1\n",
    "  - Binary file labeled type &#39;.hgt&#39;\n",
    "  - 1201x1201 grid of height postings\n",
    "  - Covers 1-degree latitude/longitude\n",
    "  - Post spacing is 3 arc-seconds\n",
    "  - Data type is unsigned 16 bit, big-endian\n",
    "  - Rows are lower to higher\n",
    "  - Columns are left to right\n",
    "  - Naming convention specifies location e.g. N39W120.hgt\n",
    "\n",
    "- Level 2\n",
    "  - Binary file labeled type &#39;.hgt&#39;\n",
    "  - 3601x3601 grid of height postings\n",
    "  - Covers 1-degree latitude/longitude\n",
    "  - Post Spacing is 1 arc-seconds\n",
    "  - Data type is unsigned 16 bit, big-endian\n",
    "  - Rows are lower to higher\n",
    "  - Columns are left to right\n",
    "  - Naming convention specifies location e.g. N39W120.hgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Visualization\n",
    "\n",
    "In this section, you will need to provide some form of visualization that summarizes or extracts a relevant characteristic or feature about the data. The visualization should adequately support the data being used. Discuss why this visualization was chosen and how it is relevant. Questions to ask yourself when writing this section:\n",
    "\n",
    "- **** _Have you visualized a relevant characteristic or feature about the dataset or input data?_\n",
    "- **** _Is the visualization thoroughly analyzed and discussed?_\n",
    "- **** _If a plot is provided, are the axes, title, and datum clearly defined?_\n",
    "\n",
    "he data is in a non-image binary format but can be converted to displayable images for \n",
    "visualization by functions in the included ‘srtm’ module. In the following discussions the term 'image' is used interchangeably \n",
    "to refer to either the raw .hgt data or a resulting displayable image, since these two formats can be converted from one to the other.\n",
    "\n",
    "\n",
    "#### Baseline Data as Images\n",
    "\n",
    "When converting the .hgt file to an image, the data is normalized to the range [0..256) to increase the contrast between\n",
    "points of lower and higher elevation.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"images/level2/N37W098-b.jpg\" width=\"256\" height=\"256\"></img></td>\n",
    "<td><img src=\"images/level2/N39W120-b.jpg\" width=\"256\" height=\"256\"></img></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:center\">N37W098 (flat) </td>\n",
    "<td style=\"text-align:center\">N39W120 (mountainous) </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "#### Subdivision of input data\n",
    "\n",
    "The full images will be subdivided into an NxN set of squares that will be the input to the model for training and test. The number of subdivisions will be determined by experimentation. The labels will be integer numbers from 0..N\\*N starting at the lower left subdivision. These actual latitude/longitude location can be determined from the label and subdivision level. \n",
    "\n",
    "After subdivision, the resulting data is again renormalized to increase the elevation contrast. That renormalization is not shown in the following two image.\n",
    "\n",
    "Example of input data subdivision:\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"images/level2/N37W098-m.jpg\" width=\"256\" height=\"256\"></img></td>\n",
    "<td><img src=\"images/level2/N39W120-m.jpg\" width=\"256\" height=\"256\"></img></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:center\">N37W098</td>\n",
    "<td style=\"text-align:center\">N39W120</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "#### Multiplication of input data\n",
    "\n",
    "The subdividing process results in only one image and label for each data point. After subdivision, each resulting  Training with just one image per label is insufficient to achieve good learning results. It can result in an overfit model that could only identify a location if the image is nearly an exact duplicate and orientation. To overcome this, for each subdivided object will be multiplied with distortion using Keras’ ImageDataGenerator function. This function creates multiple instances of each subdivided object with various rotations and offsets. \n",
    "\n",
    "\n",
    "Example modifications of one subdivision, upper left corner of N39W120, renormalized and distorted.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"images/level2/N37W098-m.jpg\" width=\"256\" height=\"256\"></img></td>\n",
    "<td><img src=\"images/level2/N39W120-m.jpg\" width=\"256\" height=\"256\"></img></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:center\">N37W098</td>\n",
    "<td style=\"text-align:center\">N39W120</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms and Techniques\n",
    "\n",
    "In this section, you will need to discuss the algorithms and techniques you intend to use for solving the problem. You should justify the use of each one based on the characteristics of the problem and the problem domain. Questions to ask yourself when writing this section:\n",
    "\n",
    "- **** _Are the algorithms you will use, including any default variables/parameters in the project clearly defined?_\n",
    "- **** _Are the techniques to be used thoroughly discussed and justified?_\n",
    "- **** _Is it made clear how the input data or datasets will be handled by the algorithms and techniques chosen?_\n",
    "- ****** http://cs231n.github.io/**\n",
    "- ****** Cross-validate or  Train/validate/test  : k-foldcross-validation less useful in CNN due to training time**\n",
    "  - **oo**** Use single train/validate/test datasets**\n",
    "- ****** Problem: Limited data set : use ImageDataGenerator**\n",
    "  - **oo**** Subdivide original &#39;image&#39;**\n",
    "  - **oo**** Use imagedatagenerator to generate N additional images of each subdivision**\n",
    "  - **oo**** Divide data individually into train/validate/train sets**\n",
    "  - **oo**** Be sure all subdivisions are represented in each set       **\n",
    "  - **oo**** Renormalize subdivisions before use**\n",
    "- ****** Problem:**\n",
    "  - **oo**** Size of subdivisions**\n",
    "  - **oo**** Large subdivisions may  generate good weights since they are too general?**\n",
    "  - **oo**** Small subdivisions may work better but take longer training time?**\n",
    "- ****** Instead of monochrome, color by height as RGB?**\n",
    "  - **oo**** Possibly increases resolution of features**\n",
    "  - **oo**** Increases dimensionality**\n",
    "  - **oo**** Training time?**\n",
    "- ******&#39;static&#39; Feature transforms**\n",
    "  - **oo**** Cartesian to polar**\n",
    "  - **oo**** Color histogram (global color)**\n",
    "  - **oo**** Histogram of oriented gradients (edge information)**\n",
    "  - **oo**** Bag of words : random patches – to – encoding**\n",
    "  - **oo**** CNN extracts feature transforms itself**\n",
    "\n",
    "### Benchmark\n",
    "\n",
    "In this section, you will need to provide a clearly defined benchmark result or threshold for comparing across performances obtained by your solution. The reasoning behind the benchmark (in the case where it is not an established result) should be discussed. Questions to ask yourself when writing this section:\n",
    "\n",
    "- **** _Has some result or value been provided that acts as a benchmark for measuring performance?_\n",
    "- **** _Is it clear how this result or value was obtained (whether by data or by hypothesis)?_\n",
    "\n",
    "- ****\n",
    "\n",
    "## III. Methodology\n",
    "\n",
    "_(approx. 3-5 pages)_\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "In this section, all of your preprocessing steps will need to be clearly documented, if any were necessary. From the previous section, any of the abnormalities or characteristics that you identified about the dataset will be addressed and corrected here. Questions to ask yourself when writing this section:\n",
    "\n",
    "- **** _If the algorithms chosen require preprocessing steps like feature selection or feature transformations, have they been properly documented?_\n",
    "- **** _Based on the _ **Data Exploration** _ section, if there were abnormalities or characteristics that needed to be addressed, have they been properly corrected?_\n",
    "- **** _If no preprocessing is needed, has it been made clear why?_\n",
    "\n",
    "### Implementation\n",
    "\n",
    "In this section, the process for which metrics, algorithms, and techniques that you implemented for the given data will need to be clearly documented. It should be abundantly clear how the implementation was carried out, and discussion should be made regarding any complications that occurred during this process. Questions to ask yourself when writing this section:\n",
    "\n",
    "- **** _Is it made clear how the algorithms and techniques were implemented with the given datasets or input data?_\n",
    "- **** _Were there any complications with the original metrics or techniques that required changing prior to acquiring a solution?_\n",
    "- **** _Was there any part of the coding process (e.g., writing complicated functions) that should be documented?_\n",
    "\n",
    "### Refinement\n",
    "\n",
    "In this section, you will need to discuss the process of improvement you made upon the algorithms and techniques you used in your implementation. For example, adjusting parameters for certain models to acquire improved solutions would fall under the refinement category. Your initial and final solutions should be reported, as well as any significant intermediate results as necessary. Questions to ask yourself when writing this section:\n",
    "\n",
    "- **** _Has an initial solution been found and clearly reported?_\n",
    "- **** _Is the process of improvement clearly documented, such as what techniques were used?_\n",
    "- **** _Are intermediate and final solutions clearly reported as the process is improved?_\n",
    "\n",
    "- ****\n",
    "\n",
    "## IV. Results\n",
    "\n",
    "_(approx. 2-3 pages)_\n",
    "\n",
    "### Model Evaluation and Validation\n",
    "\n",
    "In this section, the final model and any supporting qualities should be evaluated in detail. It should be clear how the final model was derived and why this model was chosen. In addition, some type of analysis should be used to validate the robustness of this model and its solution, such as manipulating the input data or environment to see how the model&#39;s solution is affected (this is called sensitivity analysis). Questions to ask yourself when writing this section:\n",
    "\n",
    "- **** _Is the final model reasonable and aligning with solution expectations? Are the final parameters of the model appropriate?_\n",
    "- **** _Has the final model been tested with various inputs to evaluate whether the model generalizes well to unseen data?_\n",
    "- **** _Is the model robust enough for the problem? Do small perturbations (changes) in training data or the input space greatly affect the results?_\n",
    "- **** _Can results found from the model be trusted?_\n",
    "\n",
    "### Justification\n",
    "\n",
    "In this section, your model&#39;s final solution and its results should be compared to the benchmark you established earlier in the project using some type of statistical analysis. You should also justify whether these results and the solution are significant enough to have solved the problem posed in the project. Questions to ask yourself when writing this section:\n",
    "\n",
    "- **** _Are the final results found stronger than the benchmark result reported earlier?_\n",
    "- **** _Have you thoroughly analyzed and discussed the final solution?_\n",
    "- **** _Is the final solution significant enough to have solved the problem?_\n",
    "\n",
    "- ****\n",
    "\n",
    "## V. Conclusion\n",
    "\n",
    "_(approx. 1-2 pages)_\n",
    "\n",
    "### Free-Form Visualization\n",
    "\n",
    "In this section, you will need to provide some form of visualization that emphasizes an important quality about the project. It is much more free-form, but should reasonably support a significant result or characteristic about the problem that you want to discuss. Questions to ask yourself when writing this section:\n",
    "\n",
    "- **** _Have you visualized a relevant or important quality about the problem, dataset, input data, or results?_\n",
    "- **** _Is the visualization thoroughly analyzed and discussed?_\n",
    "- **** _If a plot is provided, are the axes, title, and datum clearly defined?_\n",
    "\n",
    "### Reflection\n",
    "\n",
    "In this section, you will summarize the entire end-to-end problem solution and discuss one or two particular aspects of the project you found interesting or difficult. You are expected to reflect on the project as a whole to show that you have a firm understanding of the entire process employed in your work. Questions to ask yourself when writing this section:\n",
    "\n",
    "- **** _Have you thoroughly summarized the entire process you used for this project?_\n",
    "- **** _Were there any interesting aspects of the project?_\n",
    "- **** _Were there any difficult aspects of the project?_\n",
    "- **** _Does the final model and solution fit your expectations for the problem, and should it be used in a general setting to solve these types of problems?_\n",
    "\n",
    "### Improvement\n",
    "\n",
    "In this section, you will need to provide discussion as to how one aspect of the implementation you designed could be improved. As an example, consider ways your implementation can be made more general, and what would need to be modified. You do not need to make this improvement, but the potential solutions resulting from these changes are considered and compared/contrasted to your current solution. Questions to ask yourself when writing this section:\n",
    "\n",
    "- **** _Are there further improvements that could be made on the algorithms or techniques you used in this project?_\n",
    "- **** _Were there algorithms or techniques you researched that you did not know how to implement, but would consider using if you knew how?_\n",
    "- **** _If you used your final solution as the new benchmark, do you think an even better solution exists?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
