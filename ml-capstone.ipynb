{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "\n",
    "## Capstone Project\n",
    "\n",
    "David Howard\n",
    "\n",
    "10/15/2017\n",
    "\n",
    "## I. Definition\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "SLAM (simultaneous location and mapping) is a well-known technique for using sensor data to create a \n",
    "map of a geographic region and also keep track of the sensors location within the constructed map. \n",
    "Typically this is done with either a particle filter or a Kalman filter. One application of SLAM \n",
    "operates on an unknown region and builds up knowledge about the region via sensor measurements.\n",
    " Another application uses existing knowledge, such as maps, along with sensors, and is aimed at \n",
    " determining the sensor&#39;s location within the known region. [[1]](https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping)\n",
    "\n",
    "In reading about SLAM, I found it interesting that I had previously been trained to be a human SLAM \n",
    "algorithm when I was a helicopter pilot in the U.S. Army. We trained to navigate high speed low level \n",
    "flight by using only a topographic map and our eyes observing the terrain elevation features (no GPS allowed).\n",
    "There is a name for this technique, Nap-Of-the-Earth flight.\n",
    " [[2]](https://en.wikipedia.org/wiki/Nap-of-the-earth#Helicopter_NOE_flying)\n",
    "\n",
    "One source of potential data for use in a SLAM system is a digital elevation model, \n",
    "generically referred to as DEM. A great source of such data was created by the NASA \n",
    "Shuttle Radar Topography Mission (SRTM). In 2000, the Space Shuttle was used to create a \n",
    "digital elevation map of the entire world (minus some polar regions). The NASA SRTM data is \n",
    "publicly available for download, in various height resolutions. [[3]](https://www2.jpl.nasa.gov/srtm/mission.htm))\n",
    "\n",
    "**Problem Statement**\n",
    "\n",
    "This project will attempt to create a simple SLAM type implementation using a convolutional neural \n",
    "network to determine position within a known region. The train/test data used for learning will be \n",
    "SRTM elevation data treated as an image. Input for evaluation will be images composed of pieces of \n",
    "the same data with possible modifications such as distortion, alignment and reduced resolution. \n",
    "Output will be a predicted position of the input images.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "The outputs of the model will be predictions of the probabilities of a test image \n",
    "location matching one of the actual locations. The outputs will be a set of probabilities for each test image. \n",
    "\n",
    "The models are evaluated by comparison of the categorical_crossentropy loss of the number of epochs run. Tensorboard graphs will be used to visualize and compare the loss over various iterations of the models.\n",
    "\n",
    "The &#39;absolute&#39; accuracy will be the average of the highest predicted probability being correct over the set of test data as was computed in the dog project.\n",
    "\n",
    "test_accuracy = 100*Sum(test predictions / test labels)\n",
    "\n",
    "\n",
    "## II. Analysis\n",
    "\n",
    "\n",
    "### Data Exploration\n",
    "\n",
    "SRTM data is available online at [[4]](https://dds.cr.usgs.gov/srtm).\n",
    "\n",
    "SRTM data is available in multiple resolutions. The data is segmented into 1 latitude/longitude degree squares. \n",
    "The highest publicly available resolution is 1 arc-second per elevation posting (Level 2, ~30 meters at the equator), \n",
    "which results in a 3600x3600 matrix of elevations. The data files are actually 3601x3601 in order to fill overlaps if \n",
    "multiple cells are composed together. Other resolutions are 3 arc seconds (Level 1, ~90 meters) and (Level 0, 30 arc seconds). \n",
    "A detailed description of the data is available at [[5]](https://dds.cr.usgs.gov/srtm/version2\\_1/Documentation/SRTM\\_Topo.pdf)\n",
    "\n",
    "Organization of data\n",
    "\n",
    "- Level 1\n",
    "  - Binary file labeled type &#39;.hgt&#39;\n",
    "  - 1201x1201 grid of height postings\n",
    "  - Covers 1-degree latitude/longitude\n",
    "  - Post spacing is 3 arc-seconds\n",
    "  - Data type is unsigned 16 bit, big-endian\n",
    "  - Rows are lower to higher\n",
    "  - Columns are left to right\n",
    "  - Naming convention specifies location e.g. N39W120.hgt\n",
    "\n",
    "- Level 2\n",
    "  - Binary file labeled type &#39;.hgt&#39;\n",
    "  - 3601x3601 grid of height postings\n",
    "  - Covers 1-degree latitude/longitude\n",
    "  - Post Spacing is 1 arc-seconds\n",
    "  - Data type is unsigned 16 bit, big-endian\n",
    "  - Rows are lower to higher\n",
    "  - Columns are left to right\n",
    "  - Naming convention specifies location e.g. N39W120.hgt\n",
    "  \n",
    "Note: I found that during testing that the level 2 data used too much GPU memory on my local machine and that training times were too long ($$) on a Google Compute Engine with K80 GPU, even on small models. As a result only the level 1 data is used in testing. The images covered are the same, just at a lower resolution.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Visualization\n",
    "\n",
    "he data is in a non-image binary format but can be converted to displayable images for \n",
    "visualization by functions in the included ‘srtm’ module. In the following discussions the term 'image' is used interchangeably \n",
    "to refer to either the raw .hgt data or a resulting displayable image, since these two formats can be converted from one to the other. Because the data is single values per elevation posting, the resulting images can all be treated as monochrome. \n",
    "\n",
    "#### Baseline Data as Images\n",
    "\n",
    "When converting the .hgt file to an image, the data is normalized to the range [0..255] to increase the contrast between\n",
    "points of lower and higher elevation.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"images/level1/N37W098-b.jpg\" width=\"256\" height=\"256\"></img></td>\n",
    "<td><img src=\"images/level1/N39W120-b.jpg\" width=\"256\" height=\"256\"></img></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:center\">N37W098 (Wichita KS area, flat) </td>\n",
    "<td style=\"text-align:center\">N39W120 (Reno NV area, mountainous) </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "#### Subdivision of input data\n",
    "\n",
    "The full images will be subdivided into an NxN set of squares that will be the input to the model for training and test. The number of subdivisions will be determined by experimentation, based on training time and results. The smaller the subdivided images, the greater the resolution of predicted location. However, if the subdivided image are too small, it may results in less accurate results because of fewer distinguishing features between similar images.The labels will be integer numbers from 0..N\\*N starting at the upper left subdivision, row major order. These actual latitude/longitude location can be determined from the label and subdivision level. \n",
    "\n",
    "Example of input data subdivision N39W120, 5x5 array.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"images/level1/a0.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a1.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a2.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a3.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a4.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"images/level1/a5.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a6.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a7.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a8.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a9.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"images/level1/a10.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a11.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a12.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a13.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a14.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"images/level1/a15.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a16.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a17.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a18.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a19.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"images/level1/a20.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a21.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a22.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a23.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a24.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "#### Augmentation of input data\n",
    "\n",
    "The subdividing process results in only one image and label for each data point. After subdivision, each resulting  Training with just one image per label is insufficient to achieve good learning results. It can result in an overfit model that could only identify a location if the image is nearly an exact duplicate and orientation. To overcome this, for each subdivided object will be multiplied with distortion using Keras’ ImageDataGenerator function. This function creates multiple instances of each subdivided object with various rotations and offsets. \n",
    "\n",
    "The extent of image distortion used is subject to experimentation. One metric in the report will be distortion parameters vs probability of successful detection. \n",
    "\n",
    "\n",
    "Example of 4 modifications of one subdivision, upper left corner of N39W120, renormalized and distorted. Image on left is original. ImageDataGen parameters used were rotation=45,height_shift=0.1,width_shift=0.1.\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"images/level1/a0.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/g1.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/g2.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/g3.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/g4.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "#### Histograms \n",
    "\n",
    "Histograms of 4 sample images. X axis is pixel/height to visualize differences in height distribution. Although not directly used, it was helpful to visualize the magnitude of differences between the subdivided images.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"images/level1/a0.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a1.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a2.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a3.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "</tr>\n",
    "</table>\n",
    "<img src=\"images/level1/histogram.svg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms and Techniques\n",
    "\n",
    "The processing algorithms will leverage the techniques and general approach from the previous 'dog project'. The main difference is that the input data files have only one image for each class, rather than many images as were provided in the dog project. This limitation will be mitigated by the use of Keras' feature augmentation to take the unique individual images and vary them to provide sufficient training and test data to train the model. Initially, the feature augmentation hyperparameters will be limited in order to simplify building the model. Once a model is is working then the hyperparameters will be tuned to give more variation in the input data. \n",
    "\n",
    "The primary algorithm used will be a convolutional neural network, implemented using python and Keras using the tensorflow-gpu backend. The model will be iterated until results no further improvement is gained with an acceptable training time. \n",
    "\n",
    "The model will use some number of Convolutional layers with Dropout and MaxPooling. Variations of kernel size, padding and activation functions will be tried for these layers. The final layer be a Dense layer with GlobalAveragePooling and softmax activation. The model will be compiled with the categorical crossentropy loss function, and with variations of optimizer function evaluated.\n",
    "\n",
    "The variations of hyperparameters and training time are logged with the resulting evaluation metrics and included in the conclusions of this report.\n",
    "\n",
    "### Benchmark\n",
    "\n",
    "There is no established benchmark for this project. A benchmark is provided by using the same input to a naive 2 level feed-forward neural network. The results of this benchmark will be compared to the results from the final model.\n",
    "\n",
    " #### code\n",
    "<pre>\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape=train_X.shape[1:]))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(labels, activation='softmax'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "</pre>\n",
    "\n",
    "#### Model Graph\n",
    "<img src=\"results/benchmark/benchmark.png\" height=\"1024\" width=\"1024\"></img>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Methodology\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "Processing requires reading an input of 1200x1200 (level 1) or 3600x3600 (level2) input dataset, and producing as output the X and Y\n",
    "\n",
    "<img src=\"doc/preprocess.svg\"></img>\n",
    "\n",
    "### Implementation\n",
    "\n",
    "#### Initial Setup\n",
    "\n",
    "The basic structure of the model is based on the structure I used in \"Step 3: Create a CNN to Classify Dog Breeds (from Scratch)\" with enhancements. Using the simple Benchmark model (which had short training time), I added  Tensorboard, History and a custom progress callbacks.  I also added printout of additional metrics from the History object, including the final and best values of the loss, accuracy, validation loss and validation accurracy. Graphics of the loss and accuracy were captured from TensorBoard. Thes metric functions were added to the solution code also.\n",
    "\n",
    "Note on metrics printout: If final accuracy or final loss is worse than the corresponding best value, then the model results deteriorated in one of more of the final epochs. That is visible in the loss/accuracy graphs also.\n",
    "\n",
    "#### Platform\n",
    "\n",
    "Test were run on two platforms, depending on memory limitiations:\n",
    "\n",
    "- Intel Core i5, 16GB RAM, Nvidia GTX 1050 GPU with 2GB VRAM\n",
    "    - 2017-10-21 14:17:29.125682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\n",
    "    - name: GeForce GTX 1050\n",
    "    - major: 6 minor: 1 memoryClockRate (GHz) 1.455\n",
    "    - pciBusID 0000:01:00.0\n",
    "    - Total memory: 1.95GiB\n",
    "    - Free memory: 1.18GiB\n",
    "    - 2017-10-21 14:17:29.125711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\n",
    "    - 2017-10-21 14:17:29.125718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\n",
    "    - 2017-10-21 14:17:29.125746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0)\n",
    "    - (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0)\n",
    "\n",
    "- Google Compute Instance with Nvidia K80 GPU\n",
    "    - 2017-10-22 15:30:11.587454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\n",
    "    - name: Tesla K80\n",
    "    - major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n",
    "    - pciBusID 0000:00:04.0\n",
    "    - Total memory: 11.17GiB\n",
    "    - Free memory: 11.09GiB\n",
    "    - 2017-10-22 15:30:11.587708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\n",
    "    - 2017-10-22 15:30:11.587776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\n",
    "    - 2017-10-22 15:30:11.587850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0)\n",
    "    - (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0)\n",
    "\n",
    "\n",
    "#### Basic Approach\n",
    "\n",
    "The initial approach is to start with the Benchmark and add Layers until the results stopped improving or the training time became too long. The procedure was:\n",
    "\n",
    "- LOOP\n",
    "    - add a Conv2D layer\n",
    "    - add Dropout\n",
    "    - add MaxPooling\n",
    "    - all Conv2D layers use\n",
    "        - 'relu' activation function\n",
    "        - 'rmsprop' optimizer\n",
    "        - 'categorical_crossentropy' loss function\n",
    "        - 'same' padding\n",
    "    - use 300 epochs (this value seemed to allow all the models to converge)\n",
    "    - record training time, final loss and % accuracy\n",
    "\n",
    "\n",
    "### Refinement\n",
    "\n",
    "\n",
    "- These refinements were tried on each model (with a given set of layers)\n",
    "    - testing each iteration of the model with each of 16,32 and 64 filters\n",
    "    - testing each iteration of the model with kernel size of 3 and 5\n",
    "    \n",
    "The metrics were evaluated at each iteration of the model and the best combination of filters and kernel size were retained. Then the next layer was added and the refinement repeated on that layer. It is possible that when additional layers are added, that retrying combinations of hyperparameters on the preceding layers could get improvement. However this led to a combinatorial explosion of trial runs so at each addition of a layer, only the last layer was tested with varied hyperparaemeters.\n",
    "\n",
    "#### Problems\n",
    "\n",
    "- data set size\n",
    "    - The difference in number of points for the two different data sets is large\n",
    "    - level 1 files have 1,440,000 points\n",
    "    - level 2 files have 12,960,000 points\n",
    "    - as expected training times on the level 2 data files was substantially longer than on the level 1 files\n",
    "- GPU memory limitation\n",
    "    - the local machine used has a 2GB Nvidia GTX 1050\n",
    "    - failures due to memory exhaustion occurred with the level 2 files and large numbers of layers and filters\n",
    "    - although the Google Compute Engine K80 GPU has substantial memory, its training times were very long even on small models\n",
    "\n",
    "#### Solutions\n",
    "\n",
    "- memory limitations\n",
    "    - the data preprocessing code was cleaned up to free unused memory before running the model. this was effective in allowing more complex models and the level 2 data sets to run successfully\n",
    "    - runs that experienced failures on the local machine were run on a Google Compute instance with an Nvidia K80 GPU\n",
    "    - only the 1200x1200 data sets were used due to training times and memory requirements\n",
    "- initial refinement runs\n",
    "    - the initial refinement runs were performed locally on the level1 files to reduce training time and avoid memory exhaustion\n",
    "    - since the level1 and level2 files cover the same geographic area, they will have similar feature sets, with level1 having less resolution.\n",
    "    - the hypothesis going in is that using level 1 files to refine the model would be sufficient and that final runs could be performed on the level 2 files. The results were be compared to confirm if the that hypothesis is correct.\n",
    "    - when necessary, the level 2 runs were performed on the Google Compute instance instead of locally\n",
    "    \n",
    "\n",
    "\n",
    "#### Benchmark Results\n",
    "\n",
    "<img src=\"results/benchmark/20171021_141717.svg\"></img>\n",
    "\n",
    "<pre>\n",
    "filename  : data/level1/N39W120.hgt\n",
    "divisor   : 16\n",
    "gen_count : 15\n",
    "epochs    : 300\n",
    "model     : benchmark\n",
    "timestamp : 20171021_141717\n",
    "input shape      : (1200, 1200)\n",
    "subdivided shape : (256, 75, 75)\n",
    "normalized shape : (256, 75, 75)\n",
    "X shape          : (4096, 75, 75, 1)\n",
    "y shape          : (4096,)\n",
    "train data      X: (3072, 75, 75, 1) y: (3072, 256)\n",
    "validation data X: (256, 75, 75, 1) y: (256, 256)\n",
    "test data       X: (256, 75, 75, 1) y: (256, 256)\n",
    "_______________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "_______________________________________________________________\n",
    "conv2d_1 (Conv2D)            (None, 75, 75, 32)        320       \n",
    "global_average_pooling2d_1 ( (None, 32)                0         \n",
    "dense_1 (Dense)              (None, 256)               8448      \n",
    "_______________________________________________________________\n",
    "Total params: 8,768\n",
    "Trainable params: 8,768\n",
    "Non-trainable params: 0\n",
    "_______________________________________________________________\n",
    "final accuracy : 0.2764\n",
    "best  accuracy : 0.2806\n",
    "final loss     : 2.8188\n",
    "best loss      : 2.8188\n",
    "final val acc  : 0.3281\n",
    "best  val acc  : 0.3281\n",
    "final val loss : 2.8207\n",
    "best  val loss : 2.8207\n",
    "Avg Accuracy   : 28.9062%\n",
    "epochs         : 300\n",
    "training time  : 05:09\n",
    "</pre>\n",
    "\n",
    "#### Example Intermediate Results\n",
    "\n",
    "#### Final Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Results\n",
    "\n",
    "_(approx. 2-3 pages)_\n",
    "\n",
    "### Model Evaluation and Validation\n",
    "\n",
    "In this section, the final model and any supporting qualities should be evaluated in detail. It should be clear how the final model was derived and why this model was chosen. In addition, some type of analysis should be used to validate the robustness of this model and its solution, such as manipulating the input data or environment to see how the model&#39;s solution is affected (this is called sensitivity analysis). Questions to ask yourself when writing this section:\n",
    "\n",
    "- **** _Is the final model reasonable and aligning with solution expectations? Are the final parameters of the model appropriate?_\n",
    "- **** _Has the final model been tested with various inputs to evaluate whether the model generalizes well to unseen data?_\n",
    "- **** _Is the model robust enough for the problem? Do small perturbations (changes) in training data or the input space greatly affect the results?_\n",
    "- **** _Can results found from the model be trusted?_\n",
    "\n",
    "### Justification\n",
    "\n",
    "In this section, your model&#39;s final solution and its results should be compared to the benchmark you established earlier in the project using some type of statistical analysis. You should also justify whether these results and the solution are significant enough to have solved the problem posed in the project. Questions to ask yourself when writing this section:\n",
    "\n",
    "- **** _Are the final results found stronger than the benchmark result reported earlier?_\n",
    "- **** _Have you thoroughly analyzed and discussed the final solution?_\n",
    "- **** _Is the final solution significant enough to have solved the problem?_\n",
    "\n",
    "- ****\n",
    "\n",
    "## V. Conclusion\n",
    "\n",
    "_(approx. 1-2 pages)_\n",
    "\n",
    "### Free-Form Visualization\n",
    "\n",
    "In this section, you will need to provide some form of visualization that emphasizes an important quality about the project. It is much more free-form, but should reasonably support a significant result or characteristic about the problem that you want to discuss. Questions to ask yourself when writing this section:\n",
    "\n",
    "- **** _Have you visualized a relevant or important quality about the problem, dataset, input data, or results?_\n",
    "- **** _Is the visualization thoroughly analyzed and discussed?_\n",
    "- **** _If a plot is provided, are the axes, title, and datum clearly defined?_\n",
    "\n",
    "### Reflection\n",
    "\n",
    "In this section, you will summarize the entire end-to-end problem solution and discuss one or two particular aspects of the project you found interesting or difficult. You are expected to reflect on the project as a whole to show that you have a firm understanding of the entire process employed in your work. Questions to ask yourself when writing this section:\n",
    "\n",
    "- **** _Have you thoroughly summarized the entire process you used for this project?_\n",
    "- **** _Were there any interesting aspects of the project?_\n",
    "- **** _Were there any difficult aspects of the project?_\n",
    "- **** _Does the final model and solution fit your expectations for the problem, and should it be used in a general setting to solve these types of problems?_\n",
    "\n",
    "### Improvement\n",
    "\n",
    "In this section, you will need to provide discussion as to how one aspect of the implementation you designed could be improved. As an example, consider ways your implementation can be made more general, and what would need to be modified. You do not need to make this improvement, but the potential solutions resulting from these changes are considered and compared/contrasted to your current solution. Questions to ask yourself when writing this section:\n",
    "\n",
    "- **** _Are there further improvements that could be made on the algorithms or techniques you used in this project?_\n",
    "- **** _Were there algorithms or techniques you researched that you did not know how to implement, but would consider using if you knew how?_\n",
    "- **** _If you used your final solution as the new benchmark, do you think an even better solution exists?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
