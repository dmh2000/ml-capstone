{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "\n",
    "## Capstone Project\n",
    "\n",
    "David Howard\n",
    "\n",
    "10/15/2017\n",
    "\n",
    "## I. Definition\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "SLAM (simultaneous location and mapping) is a well-known technique for using sensor data to create a \n",
    "map of a geographic region and also keep track of the sensors location within the constructed map. \n",
    "Typically this is done with either a particle filter or a Kalman filter. One application of SLAM \n",
    "operates on an unknown region and builds up knowledge about the region via sensor measurements.\n",
    " Another application uses existing knowledge, such as maps, along with sensors, and is aimed at \n",
    " determining the sensor&#39;s location within the known region. [[1]](https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping)\n",
    "\n",
    "In reading about SLAM, I found it interesting that I had previously been trained to be a human SLAM \n",
    "algorithm when I was a helicopter pilot in the U.S. Army. We trained to navigate high speed low level \n",
    "flight by using only a topographic map and our eyes observing the terrain elevation features (no GPS allowed).\n",
    "There is a name for this technique, Nap-Of-the-Earth flight.\n",
    " [[2]](https://en.wikipedia.org/wiki/Nap-of-the-earth#Helicopter_NOE_flying)\n",
    "\n",
    "One source of potential data for use in a SLAM system is a digital elevation model, \n",
    "generically referred to as DEM. A great source of such data was created by the NASA \n",
    "Shuttle Radar Topography Mission (SRTM). In 2000, the Space Shuttle was used to create a \n",
    "digital elevation map of the entire world (minus some polar regions). The NASA SRTM data is \n",
    "publicly available for download, in various height resolutions. [[3]](https://www2.jpl.nasa.gov/srtm/mission.htm))\n",
    "\n",
    "**Problem Statement**\n",
    "\n",
    "This project will attempt to create a simple SLAM type implementation using a convolutional neural \n",
    "network to determine position within a known region. The train/test data used for learning will be \n",
    "SRTM elevation data treated as an image. Input for evaluation will be images composed of pieces of \n",
    "the same data with possible modifications such as distortion, alignment and reduced resolution. \n",
    "Output will be a predicted position of the input images.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "The outputs of the model will be predictions of the probabilities of a test image \n",
    "location matching one of the actual locations. The outputs will be a set of probabilities for each test image. \n",
    "\n",
    "The models are evaluated by comparison of the categorical_crossentropy loss of the number of epochs run. Tensorboard graphs will be used to visualize and compare the loss over various iterations of the models.\n",
    "\n",
    "The &#39;absolute&#39; accuracy will be the average of the highest predicted probability being correct over the set of test data as was computed in the dog project.\n",
    "\n",
    "test_accuracy = 100*Sum(test predictions / test labels)\n",
    "\n",
    "\n",
    "## II. Analysis\n",
    "\n",
    "\n",
    "### Data Exploration\n",
    "\n",
    "SRTM data is available online at [[4]](https://dds.cr.usgs.gov/srtm).\n",
    "\n",
    "SRTM data is available in multiple resolutions. The data is segmented into 1 latitude/longitude degree squares. \n",
    "The highest publicly available resolution is 1 arc-second per elevation posting (Level 2, ~30 meters at the equator), \n",
    "which results in a 3600x3600 matrix of elevations. The data files are actually 3601x3601 in order to fill overlaps if \n",
    "multiple cells are composed together. Other resolutions are 3 arc seconds (Level 1, ~90 meters) and (Level 0, 30 arc seconds). \n",
    "A detailed description of the data is available at [[5]](https://dds.cr.usgs.gov/srtm/version2\\_1/Documentation/SRTM\\_Topo.pdf)\n",
    "\n",
    "Organization of data\n",
    "\n",
    "- Level 1\n",
    "  - Binary file labeled type &#39;.hgt&#39;\n",
    "  - 1201x1201 grid of height postings\n",
    "  - Covers 1-degree latitude/longitude\n",
    "  - Post spacing is 3 arc-seconds\n",
    "  - Data type is unsigned 16 bit, big-endian\n",
    "  - Rows are lower to higher\n",
    "  - Columns are left to right\n",
    "  - Naming convention specifies location e.g. N39W120.hgt\n",
    "\n",
    "- Level 2\n",
    "  - Binary file labeled type &#39;.hgt&#39;\n",
    "  - 3601x3601 grid of height postings\n",
    "  - Covers 1-degree latitude/longitude\n",
    "  - Post Spacing is 1 arc-seconds\n",
    "  - Data type is unsigned 16 bit, big-endian\n",
    "  - Rows are lower to higher\n",
    "  - Columns are left to right\n",
    "  - Naming convention specifies location e.g. N39W120.hgt\n",
    "  \n",
    "Note: I found that during testing that the level 2 data used too much GPU memory on my local machine and that training times were too long ($$) on a Google Compute Engine with K80 GPU, even on small models. As a result only the level 1 data is used in testing. The images covered are the same, just at a lower resolution.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Visualization\n",
    "\n",
    "he data is in a non-image binary format but can be converted to displayable images for \n",
    "visualization by functions in the included ‘srtm’ module. In the following discussions the term 'image' is used interchangeably \n",
    "to refer to either the raw .hgt data or a resulting displayable image, since these two formats can be converted from one to the other. Because the data is single values per elevation posting, the resulting images can all be treated as monochrome. \n",
    "\n",
    "#### Baseline Data as Images\n",
    "\n",
    "When converting the .hgt file to an image, the data is normalized to the range [0..255] to increase the contrast between\n",
    "points of lower and higher elevation.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"images/level1/N37W098-b.jpg\" width=\"256\" height=\"256\"></img></td>\n",
    "<td><img src=\"images/level1/N39W120-b.jpg\" width=\"256\" height=\"256\"></img></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:center\">N37W098 (Wichita KS area, flat) </td>\n",
    "<td style=\"text-align:center\">N39W120 (Reno NV area, mountainous) </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "#### Subdivision of input data\n",
    "\n",
    "The full images will be subdivided into an NxN set of squares that will be the input to the model for training and test. The number of subdivisions will be determined by experimentation, based on training time and results. The smaller the subdivided images, the greater the resolution of predicted location. However, if the subdivided image are too small, it may results in less accurate results because of fewer distinguishing features between similar images.The labels will be integer numbers from 0..N\\*N starting at the upper left subdivision, row major order. These actual latitude/longitude location can be determined from the label and subdivision level. \n",
    "\n",
    "Example of input data subdivision N39W120, 5x5 array.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"images/level1/a0.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a1.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a2.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a3.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a4.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"images/level1/a5.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a6.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a7.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a8.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a9.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"images/level1/a10.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a11.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a12.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a13.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a14.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"images/level1/a15.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a16.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a17.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a18.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a19.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"images/level1/a20.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a21.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a22.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a23.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a24.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "#### Augmentation of input data\n",
    "\n",
    "The subdividing process results in only one image and label for each data point. After subdivision, each resulting  Training with just one image per label is insufficient to achieve good learning results. It can result in an overfit model that could only identify a location if the image is nearly an exact duplicate and orientation. To overcome this, for each subdivided object will be multiplied with distortion using Keras’ ImageDataGenerator function. This function creates multiple instances of each subdivided object with various rotations and offsets. \n",
    "\n",
    "The extent of image distortion used is subject to experimentation. One metric in the report will be distortion parameters vs probability of successful detection. \n",
    "\n",
    "\n",
    "Example of 4 modifications of one subdivision, upper left corner of N39W120, renormalized and distorted. Image on left is original. ImageDataGen parameters used were rotation=45,height_shift=0.1,width_shift=0.1.\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"images/level1/a0.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/g1.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/g2.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/g3.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/g4.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "#### Histograms \n",
    "\n",
    "Histograms of 4 sample images. X axis is pixel/height to visualize differences in height distribution. Although not directly used, it was helpful to visualize the magnitude of differences between the subdivided images.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"images/level1/a0.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a1.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a2.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/level1/a3.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "</tr>\n",
    "</table>\n",
    "<img src=\"images/level1/histogram.svg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms and Techniques\n",
    "\n",
    "The processing algorithms will leverage the techniques and general approach from the previous 'dog project'. The main difference is that the input data files have only one image for each class, rather than many images as were provided in the dog project. This limitation will be mitigated by the use of Keras' feature augmentation to take the unique individual images and vary them to provide sufficient training and test data to train the model. Initially, the feature augmentation hyperparameters will be limited in order to simplify building the model. Once a model is is working then the hyperparameters will be tuned to give more variation in the input data. \n",
    "\n",
    "The primary algorithm used will be a convolutional neural network, implemented using python and Keras using the tensorflow-gpu backend. The model will be iterated until results no further improvement is gained with an acceptable training time. \n",
    "\n",
    "The model will use some number of Convolutional layers with Dropout and MaxPooling. Variations of kernel size, padding and activation functions will be tried for these layers. The final layer be a Dense layer with GlobalAveragePooling and softmax activation. The model will be compiled with the categorical crossentropy loss function, and with variations of optimizer function evaluated.\n",
    "\n",
    "The variations of hyperparameters and training time are logged with the resulting evaluation metrics and included in the conclusions of this report.\n",
    "\n",
    "### Benchmark\n",
    "\n",
    "There is no established benchmark for this project. A benchmark is provided by using the same input to a naive 2 level feed-forward neural network. The results of this benchmark will be compared to the results from the final model.\n",
    "\n",
    " #### code\n",
    "<pre>\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape=train_X.shape[1:]))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(labels, activation='softmax'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "</pre>\n",
    "\n",
    "#### Model Graph\n",
    "<img src=\"results/benchmark/benchmark.png\" height=\"768\" width=\"768\"></img>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Methodology\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "Processing requires reading an input of 1200x1200 (level 1) or 3600x3600 (level2) input dataset, and producing as output the X and Y\n",
    "\n",
    "<img src=\"doc/preprocess.svg\"></img>\n",
    "\n",
    "### Implementation\n",
    "\n",
    "#### Initial Setup\n",
    "\n",
    "The basic structure of the model is based on the structure I used in \"Step 3: Create a CNN to Classify Dog Breeds (from Scratch)\" with enhancements. Using the simple Benchmark model (which had short training time), I added  Tensorboard, History and a custom progress callbacks.  I also added printout of additional metrics from the History object, including the final and best values of the loss, accuracy, validation loss and validation accurracy. Graphics of the loss and accuracy were captured from TensorBoard. Thes metric functions were added to the solution code also.\n",
    "\n",
    "Note on metrics printout: If final accuracy or final loss is worse than the corresponding best value, then the model results deteriorated in one of more of the final epochs. That is visible in the loss/accuracy graphs also.\n",
    "\n",
    "#### Platform\n",
    "\n",
    "Test were run on two platforms, depending on memory limitiations:\n",
    "\n",
    "- Intel Core i5, 16GB RAM, Nvidia GTX 1050 GPU with 2GB VRAM\n",
    "    - 2017-10-21 14:17:29.125682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\n",
    "    - name: GeForce GTX 1050\n",
    "    - major: 6 minor: 1 memoryClockRate (GHz) 1.455\n",
    "    - pciBusID 0000:01:00.0\n",
    "    - Total memory: 1.95GiB\n",
    "    - Free memory: 1.18GiB\n",
    "    - 2017-10-21 14:17:29.125711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\n",
    "    - 2017-10-21 14:17:29.125718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\n",
    "    - 2017-10-21 14:17:29.125746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0)\n",
    "    - (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0)\n",
    "\n",
    "- Google Compute Instance with Nvidia K80 GPU\n",
    "    - 2017-10-22 15:30:11.587454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\n",
    "    - name: Tesla K80\n",
    "    - major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n",
    "    - pciBusID 0000:00:04.0\n",
    "    - Total memory: 11.17GiB\n",
    "    - Free memory: 11.09GiB\n",
    "    - 2017-10-22 15:30:11.587708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\n",
    "    - 2017-10-22 15:30:11.587776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\n",
    "    - 2017-10-22 15:30:11.587850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0)\n",
    "    - (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0)\n",
    "\n",
    "\n",
    "#### Basic Approach\n",
    "\n",
    "The initial approach is to start with the Benchmark and add Layers until the results stopped improving or the training time became too long. The procedure was:\n",
    "\n",
    "- LOOP\n",
    "    - add a Conv2D layer\n",
    "    - add Dropout\n",
    "    - add MaxPooling\n",
    "    - all Conv2D layers use\n",
    "        - 'relu' activation function\n",
    "        - 'rmsprop' optimizer\n",
    "        - 'categorical_crossentropy' loss function\n",
    "        - 'same' padding\n",
    "    - use 300 epochs (this value seemed to allow all the models to converge)\n",
    "    - record training time, final loss and % accuracy\n",
    "\n",
    "\n",
    "### Refinement\n",
    "\n",
    "\n",
    "- These refinements were tried on each model (with a given set of layers)\n",
    "    - testing each iteration of the model with each of 16,32 and 64 filters\n",
    "    - testing each iteration of the model with kernel size of 3 and 5\n",
    "    \n",
    "The metrics were evaluated at each iteration of the model and the best combination of filters and kernel size were retained. Then the next layer was added and the refinement repeated on that layer. It is possible that when additional layers are added, that retrying combinations of hyperparameters on the preceding layers could get improvement. However this led to a combinatorial explosion of trial runs so at each addition of a layer, only the last layer was tested with varied hyperparaemeters.\n",
    "\n",
    "#### Problems\n",
    "\n",
    "- data set size\n",
    "    - The difference in number of points for the two different data sets is large\n",
    "    - level 1 files have 1,440,000 points\n",
    "    - level 2 files have 12,960,000 points\n",
    "    - as expected training times on the level 2 data files was substantially longer than on the level 1 files\n",
    "- GPU memory limitation\n",
    "    - the local machine used has a 2GB Nvidia GTX 1050\n",
    "    - failures due to memory exhaustion occurred with the level 2 files and large numbers of layers and filters\n",
    "    - although the Google Compute Engine K80 GPU has substantial memory, its training times were very long even on small models\n",
    "\n",
    "#### Solutions\n",
    "\n",
    "- memory limitations\n",
    "    - the data preprocessing code was cleaned up to free unused memory before running the model. this was effective in allowing more complex models and the level 2 data sets to run successfully\n",
    "    - runs that experienced failures on the local machine were run on a Google Compute instance with an Nvidia K80 GPU\n",
    "    - only the 1200x1200 data sets were used due to training times and memory requirements\n",
    "- initial refinement runs\n",
    "    - the initial refinement runs were performed locally on the level1 files to reduce training time and avoid memory exhaustion\n",
    "    - since the level1 and level2 files cover the same geographic area, they will have similar feature sets, with level1 having less resolution.\n",
    "    - the hypothesis going in is that using level 1 files to refine the model would be sufficient and that final runs could be performed on the level 2 files. The results were be compared to confirm if the that hypothesis is correct.\n",
    "    - when necessary, the level 2 runs were performed on the Google Compute instance instead of locally\n",
    "    \n",
    "\n",
    "#### Level 2 Results For Model 1 on Google Compute with K80 GPU\n",
    "\n",
    "This result is from level2 3600x3600 data and had a very long training time. This is why the remaining tests were performed with the 1200x1200 data. The level2 data was too large to run on my local machine 1050 GPU.\n",
    "\n",
    "<pre>\n",
    "timestamp : 20171022_152912\n",
    "config    : config/model1/level2-N39W120.json\n",
    "datafile  : data/level2/N37W098.hgt\n",
    "divisor   : 16\n",
    "augments  : 15\n",
    "epochs    : 300\n",
    "model     : model1\n",
    "input shape      : (3600, 3600)\n",
    "subdivided shape : (256, 225, 225)\n",
    "normalized shape : (256, 225, 225)\n",
    "X shape          : (4096, 225, 225, 1)\n",
    "y shape          : (4096,)\n",
    "...\n",
    "epochs         : 300\n",
    "<strong>training time  : 76:07</strong>\n",
    "</pre>\n",
    "\n",
    "\n",
    "#### Benchmark Results\n",
    "\n",
    "<img src=\"results/benchmark/20171021_141717.svg\"></img>\n",
    "\n",
    "<pre>\n",
    "timestamp : 20171023_074451\n",
    "config    : config/benchmark.json\n",
    "datafile  : data/level1/N39W120.hgt\n",
    "divisor   : 16\n",
    "augments  : 15\n",
    "epochs    : 300\n",
    "model     : benchmark\n",
    "input shape      : (1200, 1200)\n",
    "subdivided shape : (256, 75, 75)\n",
    "normalized shape : (256, 75, 75)\n",
    "X shape          : (4096, 75, 75, 1)\n",
    "y shape          : (4096,)\n",
    "model init\n",
    "train data      X: (3072, 75, 75, 1) y: (3072, 256)\n",
    "validation data X: (256, 75, 75, 1) y: (256, 256)\n",
    "test data       X: (256, 75, 75, 1) y: (256, 256)\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv2d_1 (Conv2D)            (None, 75, 75, 32)        320       \n",
    "_________________________________________________________________\n",
    "global_average_pooling2d_1 ( (None, 32)                0         \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 256)               8448      \n",
    "=================================================================\n",
    "Total params: 8,768\n",
    "Trainable params: 8,768\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "loss : 3.4349829852581024\n",
    "acc  : 0.12890625\n",
    "best  accuracy : 0.1335\n",
    "final loss     : 3.4889\n",
    "best loss      : 3.4889\n",
    "final val acc  : 0.1563\n",
    "best  val acc  : 0.1758\n",
    "final val loss : 3.4913\n",
    "best  val loss : 3.4733\n",
    "Avg Accuracy   : 12.8906%\n",
    "epochs         : 300\n",
    "training time  : 05:09\n",
    "done\n",
    "\n",
    "</pre>\n",
    "\n",
    "#### Model 1 : 2 Conv2D layers\n",
    "\n",
    "##### Model Graph\n",
    "<img src=\"doc/model1.png\" height=\"768\" width=\"768\"></img>\n",
    "\n",
    "##### Performance\n",
    "<img src=\"results/xxx/xxx.svg\"></img>\n",
    "\n",
    "##### Output\n",
    "\n",
    "<pre>\n",
    "xxx.txt\n",
    "</pre>\n",
    "\n",
    "#### Model 2 : 3 Conv2D layers, 16 filters, kernel=3\n",
    "\n",
    "##### Model Graph\n",
    "<img src=\"doc/model2.png\" height=\"768\" width=\"768\"></img>\n",
    "\n",
    "##### Performance\n",
    "<img src=\"results/xxx/xxx.svg\"></img>\n",
    "\n",
    "##### Output\n",
    "\n",
    "<pre>\n",
    "xxx.txt\n",
    "</pre>\n",
    "\n",
    "\n",
    "#### Model 3 :  3 Conv2D layers, 32 filters, kernel=5\n",
    "\n",
    "##### Model Graph\n",
    "<img src=\"doc/model3.png\" height=\"768\" width=\"768\"></img>\n",
    "\n",
    "##### Performance\n",
    "<img src=\"results/xxx/xxx.svg\"></img>\n",
    "\n",
    "##### Output\n",
    "\n",
    "<pre>\n",
    "xxx.txt\n",
    "</pre>\n",
    "\n",
    "\n",
    "\n",
    "#### Model 4 :  4 Conv2D layers, 32 filters, kernel=5\n",
    "\n",
    "##### Model Graph\n",
    "<img src=\"doc/model4.png\" height=\"768\" width=\"768\"></img>\n",
    "\n",
    "\n",
    "##### Performance\n",
    "<img src=\"results/xxx/xxx.svg\"></img>\n",
    "\n",
    "##### Output\n",
    "\n",
    "<pre>\n",
    "xxx.txt\n",
    "</pre>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Results\n",
    "\n",
    "### Model Evaluation and Validation\n",
    "\n",
    "The final model (Model 4) was derived by starting with the Benchmark and successively adding layers with varying hyperparameters. The results of Model 4 seemed too good to be true. The successive loss/accuracy scores did indicates an improvement in each model. My guess is that the dataset is too limited. It has 256 base images, with augmentation inscreaing that to 4096 images total. The lower resolution level 1 (1200x1200) data files, divided into 16x16 subimages, resulted in each image being 75x75 pixels. The augmented images had relatively small modification hyperparameters so those images are close to the original. \n",
    "\n",
    "The change from 16 filters, kernel=3 in model 2, to 32 filters, kernel=5 in models 3 and 4 made a significant difference. Both models 3 and 4 converged very quickly and did not need nearly as many training epochs as were allocated. \n",
    "\n",
    "The measured loss and accuracy was best in model 3. Model 4 regressed a bit. Although it is not shown here , the models were run on both the N39W120 dataset and the N37W098 datasets. Both datasets reflected the regression in model 4 from model 3. The N37W098 results are in the 'results' directory. (The .txt file in each timestampted results folder shows which dataset was used.)\n",
    "\n",
    "A possible explanation of the regression is that, although in all cases there was almost no divergence between the training curves and the validation curves, the model 4 validation loss showed a slight upward slope that was not present in the model 3 curves. This could indicate that model 4 was beginning to overfit.\n",
    "\n",
    "#### Results from N39W120 (mountainous terrain)\n",
    "- Benchmark\n",
    "    - loss : 3.434\n",
    "    - acc  : 0.128\n",
    "    \n",
    "- Model 1\n",
    "    - loss : 0.674\n",
    "    - acc  : 0.796\n",
    "    \n",
    "- Model 2\n",
    "    - loss : 0.464\n",
    "    - acc  : 0.871\n",
    "\n",
    "- Model 3\n",
    "    - loss : <strong>0.110</strong>\n",
    "    - acc  : <strong>0.972</strong>\n",
    "\n",
    "- Model 4\n",
    "    - loss : <strong>0.285</strong>\n",
    "    - acc  : <strong>0.949</strong>\n",
    "\n",
    "#### Results from N37098 (flat terrain)\n",
    "- Model 1\n",
    "    - loss : 0.674\n",
    "    - acc  : 0.796\n",
    "    \n",
    "- Model 2\n",
    "    - loss : 0.464\n",
    "    - acc  : 0.871\n",
    "\n",
    "- Model 3\n",
    "    - loss : <strong>0.110</strong>\n",
    "    - acc  : <strong>0.972</strong>\n",
    "\n",
    "- Model 4\n",
    "    - loss : <strong>0.285</strong>\n",
    "    - acc  : <strong>0.949</strong>\n",
    "    \n",
    "### Justification\n",
    "\n",
    "The best solution was with model 3. It was much better than the benchmark. Much of this improvement could be due to the simplicity of the input data. It appears the input data happens to be very friendly to a simple convolutional neural network. \n",
    "\n",
    "One question to be answered is whether the predictions on the test data are real. Some visualization is needed to confirm that. Using model 3, the code was modified to store the base images before subdivision, and the test image set. Then the model was re-run and some examples shown here of test images, both those where the prediction matched the label, and those that did not match. Those images are shown in the next section.\n",
    "\n",
    "\n",
    "## V. Conclusion\n",
    "\n",
    "\n",
    "### Free-Form Visualization\n",
    "\n",
    "The images in the following two tables show examples from a run of Model 1 where some test images were predicted correctly and some images were predicted incorrectly. The 'Input' is the original image before augmentation. The 'Predicted' is one of the augmented images matching the predicted label. These images are stored in the github project under directory 'images/model1'. \n",
    "\n",
    "#### Example Base vs Test images for Model 3 where the input label and predicted label match\n",
    "\n",
    "<table style=\"td\">\n",
    "<tr>\n",
    "<td></td><td></td><td></td>\n",
    "<td><strong>INPUT LABEL</strong></td>\n",
    "<td></td><td></td><td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>1</td><td>51</td><td>101</td><td>131</td><td>191</td><td>201</td><td>255</td>\n",
    "<tr>\n",
    "<tr>\n",
    "<td><img src=\"images/model1/base001.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/model1/base051.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/model1/base101.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/model1/base131.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/model1/base191.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/model1/base201.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/model1/base255.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>1</td><td>51</td><td>101</td><td>131</td><td>191</td><td>201</td><td>255</td>\n",
    "<tr>\n",
    "<td></td><td></td><td></td>\n",
    "<td>PREDICTED LABEL</td>\n",
    "<td></td><td></td><td></td>\n",
    "</tr>\n",
    "<td>1</td><td>51</td><td>101</td><td>131</td><td>191</td><td>201</td><td>255</td>\n",
    "<tr>\n",
    "<td><img src=\"images/model1/test001.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/model1/test051.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/model1/test101.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/model1/test131.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/model1/test191.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/model1/test201.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/model1/test255.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "#### Example Base vs Test images for Model 3 where the input label and predicted label did not match\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td></td><td></td><td></td>\n",
    "<td>INPUT LABEL</td>\n",
    "<td></td><td></td><td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>24</td><td>69</td><td>123</td><td>133</td><td>174</td><td>177</td><td>188</td>\n",
    "<tr>\n",
    "<tr>\n",
    "<td><img src=\"images/model1/base024.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/model1/base069.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/model1/base123.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/model1/base133.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/model1/base174.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/mode11/base177.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/model1/base188.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td></td><td></td><td></td>\n",
    "<td>PREDICTED LABEL</td>\n",
    "<td></td><td></td><td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>221</td><td>164</td><td>109</td><td>219</td><td>195</td><td>87</td><td>212</td>\n",
    "<tr>\n",
    "<tr>\n",
    "<td><img src=\"images/model1/test221.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/model1/test164.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/model1/test109.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/model1/test219.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/model1/test195.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/model1/test087.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "<td><img src=\"images/model1/test212.jpg\" width=\"96\" height=\"96\"></img></td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "### Reflection\n",
    "\n",
    "I think my original proposal had a bit too much scope. I followed it, but I found I did not have time to do everything I wanted to do. These results would just be a first step to creating anything resembling a useful system for determining position. While the recognition of images worked well, the simple division in NxN sections is not good enough. A real system would need to take overlapping sections into account. It would need a process that detected subsections within a larger image. \n",
    "\n",
    "At this point I do not know enough about the innards of the various neural net layers and how they could be used to improve a model. It just seems like a combinatorial problem, trying things until something works. That is clearly not a good approach. I did learn a lot about using Numpy to process data and refined my Python skills a bit. I think at this point I could serve as an implementer of a neural net processing pipeline if someone with more theoretical knowledge specified the model itself.\n",
    "\n",
    "####  data preprocessing \n",
    "\n",
    "This was the majority of coding time spent. I had to figure out how to to read the SRTM data, convert it to numpy arrays, write it as images and format it as tensors for the model. Much of the time was spent refactoring the preprocessing code to provide a clean pipeline. I learned a lot about using numpy efficiently. My first cut of code used explicit loops (I'm a C programmer), which I then refactored to use slicing and numpy array functions. My lib.srtm module probably duplicated existing functions in numpy, scipy and PIL but doing it myself helped me improve my skillset.\n",
    "\n",
    "#### model processing\n",
    "\n",
    "While the models themselves were very simple and did not require much coding, I spent a lot of time constructing and refactoring the processing pipeline to be general enough to run different models without code change, using the .json configuration file. \n",
    "\n",
    "#### directory structure\n",
    "\n",
    "I did a lot of refactoring of the project directory structure, both for the data preprocessing and saving the results. I ended up with a lot of directories but I think it is well organized.\n",
    "\n",
    "#### Numpy, Tensorboard, Keras, Google Compute Engine\n",
    "\n",
    "I learned a lot about using Python and Numpy. I used Tensorboard a lot during development. I just touched the surface on Keras and designing models in general. I learned how to use Google Compute Engine, and I found it much easier to use then the Amazon alternative. Unfortunately the GPU support is pretty slow and expensive.\n",
    "\n",
    "### Improvement\n",
    "\n",
    "The application could be improved by segmenting the individual .hgt files into smaller subsegments, using a system capable of handling the 3600x3600 resolution, along with training the model over a collection of the .hgt files instead of one at a time. Smaller subsegments with higher resolution would make the position resolution more precise. The 1200x1200 resolution files might not provide enough differentiation between similar images. Grouping multiple .hgt files with a proper labeling convention a much larger coverage area could be trained in one pass. The current approach of one .hgt at a time is a limiting factor."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
